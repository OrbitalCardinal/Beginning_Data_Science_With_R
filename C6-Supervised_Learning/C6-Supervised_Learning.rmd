---
title: "Chapter 6 - Supervised Learning"
output: html_document
author: "Edson Raul Cepeda Marquez"
---

### Machine Learning

Machine Learning is the discipline of developing and applying models and algorithms for learning from data.
Machine Learning concerns learning from data, you use a generic algorithm that you feed examples of solutions
to, and let it learn how to solve the problem from those examples.

### Supervised Learning

Supervised Learning is used when you have variables you want to predict using other variables.
For the simplest case of supervised learning, we have one response variable, $y$, and one input variable, $x$, and we want to figure out 
a function, $f$, mapping the input to output, i.e., so that $y=f(x)$.
What we have to work with is example data of matching $x$ and $y$.

### Regression versus Classification

There are two types of supervised learning: regression and classification. Regression is used when the output variable we try to target is a number.
Classification is used when we try to target categorical variables.

Take linear regression, $y = \alpha x + \beta$. It is a regression because the variable we are trying to target is a number.
The parameterized class of functions $f_\theta$, are all lines. If we let $\theta = \theta_1, \theta_0$ and $\alpha = \theta_1, \beta = \theta_0$
then $y(\theta) = f(x;\theta) = \theta_1 x + \theta_0$. Fitting a linear model consist of finding the best $\theta$, where *best* is defined as the 
$\theta$ that gets **$y$**$(\theta)$ closes to **$t$**. The distance used in linear regression is the squared Euclidean distance

$$
|y^{(\theta)} - t|^2 = \sum_{i=1}^n (\theta_1 x_i + \theta_0 - t_i)^2
$$ .

For an example of classfication, assume that the targets $t_i$ are binary, encoded as 0 and 1, but that the input variables $x_i$ are still real numbers.
A common way of defining tha mapping function $f(-;\theta)$ is to let it map $x$ to the unit interval $[0,1]$ and interpret the resulting $y(\theta)$ as 
the probability that $t$ is 1.
In a classification setting, you would then predict 0 if $f(x;\theta) < 0.5$ and predict 1 if $f(x;\theta) > 0.5$.
In linear classification , the function $f_\theta$ could look like this:

$$
f(x;\theta) = \sigma (\theta_1 x + \theta_0)
$$

Where $\sigma$ is a sigmoid function (a function mapping $R \rightarrow [0,1]$ that is "S-shaped").
A common choice of $\sigma$ is the logistic function $\sigma : z \longmapsto \frac{1}{1 + e^{-z'}}$ in which case we call the fitting $f(-;\theta)$ *logistic regression*. 

#### Linear Regression
If we take a simple linear regression, $f_\theta = \theta_1 x + \theta_0$, we need the function lm().

For example, considering the speed as the $x$ value and the distance as the $y$ for the *cars* dataset.

```{r}
library(magrittr)
library(ggplot2)
```
```{r}
cars %>% head
```

We can see that there is a very clear linear relation between speed and distance:

```{r}
cars %>% ggplot(aes(x = speed, y = dist)) +
    geom_point() +
    geom_smooth(method = "lm")
```

`geom_smooth()` will also plot the uncertainty of the fit. This is the gray area in the plot, is the area were the line is likely to be.

To actually fit the data and get information about the fit, we use the `lm()` function with the model specification, `dist ~ speed`, and we can use the `summary()` function to see information about the fit.

```{r}
cars %>% lm(dist ~ speed, data = .) %>% summary
```

Or we can use the `coefficients()` function to get the point estimates and the `confint()` function to confidence interval for the parameters.

```{r}
cars %>% lm(dist ~ speed, data = .) %>% coefficients
cars %>% lm(dist ~ speed, data = .) %>% confint
```

Here, (`Intercept`) is $\theta_0$ and `speed` is $\theta_1$.

##### Manually getting the linear regression

Plotting the lines $y = \theta_1 x$ for different choices of $\theta$
```{r}
predict_dist <- function(speed, theta_1)
    data.frame(speed = speed,
               dist = theta_1 * speed,
               theta = as.factor(theta_1))

cars %>% ggplot(aes(x = speed, y = dist, colour = theta)) +
    geom_point(colour = "black") +
    geom_line(data = predict_dist(cars$speed,2)) +
    geom_line(data = predict_dist(cars$speed,3)) +
    geom_line(data = predict_dist(cars$speed,4)) +
    scale_color_discrete(name=expression(theta[1]))

```

Plotting the error against different choices of $\theta_1$.

```{r}
thetas <- seq(0,5, length.out = 50)
fitting_error <- Vectorize(
    function(theta)
        sum((theta * cars$speed - cars$dist)**2)
)

data.frame(thetas = thetas, errors =  fitting_error(thetas)) %>%
    ggplot(aes(x = thetas, y = errors)) +
    geom_line() +
    xlab(expression(theta[1])) + ylab("")
```

#### Logistic Regression (Classification)

Consider binary clasiffication and logistic regression.
We can use the breat cancer data from the `mlbench` library and as if the clump thickness has an effect on the risk
of a tumor being malignant.
We want to see if we can predict the `Class` variable from the `Cl.thickness` variable.

```{r}
library(mlbench)
library(knitr)
data("BreastCancer")
BreastCancer %>% head
```

We can plot the data against the fit. 
Since the malignant status is either 0 or 1, the points would overlap but if we add a little jitter to the plot we can still se them,
and if we make them slightly transparent, we can see the density of the points.

```{r}
BreastCancer %>%
    ggplot(aes(x = Cl.thickness, y = Class)) +
    geom_jitter(height = 0.05, width = 0.3, alpha = 0.4)
```

For classification we still specify the prediction function $y=f(x)$ using the formula `y ~ x`.
The outcome parameter for `y ~ x` is just binary now.

The breast cancer dataset consider the clump thickness as ordered factors.
Generally, it is no advisable to directly translate categorical data into numeric data, it is okay in this case.
Using the function `as.numeric()` will do this.
Another safer aproach is to first translate the factor into strings and then into numbers.

The second problem is that the `glm()` function expects the response variable to be numerical, coding classes like 0 or 1, 
while `BreastCancer` data encodes the classes as a factor.

```{r}
library(dplyr)
BreastCancer %>%
    mutate(Cl.thickness.numeric = as.numeric(as.character(Cl.thickness))) %>% 
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    ggplot(aes(x = Cl.thickness.numeric, y = IsMalignant)) +
    geom_jitter(height = 0.05, width = 0.3, alpha = 0.4) +
    geom_smooth(method = "glm",
                method.args = list(family = "binomial"))

```

To actually get the fitted object, we use `glm()` like we used `lm()` for the linear regression.

```{r}
BreastCancer %>% 
    mutate(Cl.thickness.numeric = 
                as.numeric(as.character(Cl.thickness))) %>%
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    glm(IsMalignant ~ Cl.thickness.numeric,
        family = "binomial",
        data = .)
```

#### Model Matrices and Formula

Most statistical models and machine learning algorithm creates maps not from single value, but form a vector.
What th elinear model actually sees is a matrix $\mathbf{x}$ considering that the data to fit is  $(\mathbf{x}. \mathbf{t})$,
so we'll call that $X$.
This matrix is known as the *model matrix*. 

It has a row per value in $\mathbf{x}$ and it has two columns, on for the intercept and one for the $x$ values.

$$ 
X =
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
1 & x_3 \\
\vdots & \vdots \\
1 & x_n
\end{bmatrix}
$$

We can see what mode matrix R generates for a given dataset and formula using
`model.matrix()` function. For the `cars` data, if we want to fir `dist` versus `speed` we get this:

```{r}
cars %>%
    model.matrix(dist ~ speed, data = .) %>%
    head(5)
```

For linear regression, the map is a pretty simple one. If we let $\theta = (\theta_0, \theta_1)$ then it is 
just multiplying that with the model matrix $X$.

$$
\theta^T X = (\theta_0,\theta_1)
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
1 & x_3 \\
\vdots & \vdots \\
1 & x_n
\end{bmatrix}
= 
\begin{bmatrix}
\theta_0 + \theta_1 x_1 \\
\theta_0 + \theta_1 x_2 \\
\theta_0 + \theta_1 x_3 \\
\vdots \\
\theta_0 + \theta_1 x_n \\
\end{bmatrix}
$$

The combination of formulas and model matrices is a powerful tool for specifying model.
Since all the algorithms we use for fitting data works on model matrices anyway, there is no reason to hold back on how complex formulas to give them.
If you want to fit more than one parameter, no problem. You just give write `y ~ x + z` and the model matrix will have three columns.

$$
X =
\begin{bmatrix}
1 & x_1 & z_1 \\
1 & x_2 & z_2 \\
1 & x_3 & z_3 \\
\vdots & \vdots & \vdots \\
1 & x_n & z_n \\
\end{bmatrix}
$$

If we wanted to fit the breast cancer data to both cell thickness and cell size, we can do that just by adding both explanatory variables in the formula.

```{r}
BreastCancer %>%
    mutate(Cl.thickness.numeric =
            as.numeric(as.character(Cl.thickness)),
           Cell.size.numeric = 
            as.numeric(as.character(Cell.size))) %>%
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    model.matrix(IsMalignant ~ Cl.thickness.numeric + Cell.size.numeric,
                 data = .) %>%
    head(5)
```

Then the generalized linear model fitting function will happily work with that:

```{r}
BreastCancer %>%
    mutate(Cl.thickness.numeric =
            as.numeric(as.character(Cl.thickness)),
           Cell.size.numeric = 
            as.numeric(as.character(Cell.size))) %>%
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    glm(IsMalignant ~ Cl.thickness.numeric + Cell.size.numeric,
        family = "binomial",
        data = .)
```

If you want to include interactions between your parameters, you specify that using * instead of +:

```{r}
BreastCancer %>%
    mutate(Cl.thickness.numeric =
            as.numeric(as.character(Cl.thickness)),
           Cell.size.numeric = 
            as.numeric(as.character(Cell.size))) %>%
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    model.matrix(IsMalignant ~ Cl.thickness.numeric * Cell.size.numeric,
                 data = .) %>%
    head(5)
```

If you want to use all the variables in your data excpet the response variable, you can even use the formula `y ~ .` where the `.` wiil give tou all parameters in your data except `y`.

We can transform the data before give it to our learning algorithms. 
We use a function $\phi$. It is called phi because we call what it produces *features* of our data and the point of it is to pull
out relevant features of the data to give to the learning algorithm.
It maps from vectors to vectors, so we can use it to transform each row in your raw data into the rows of the model matrix, whigh we will then call $\phi$ instead of $X$.

$$
\phi = 
\begin{bmatrix}
- \phi (\mathbf{x}_1) - \\
- \phi (\mathbf{x}_2) - \\
- \phi (\mathbf{x}_3) - \\
\cdots \\
- \phi (\mathbf{x}_n) - \\
\end{bmatrix}
$$

To train the `cars`data but fitting a polynomial instead of a line.
Considering $d$ denotes breaking distance and $s$ the speed, then we want to fit $d = \theta_0 + \theta_1 s + \theta_2 s^2 + \cdots + \theta_n s^n$.
We'll just do $n=2$ so we want to fit a second-degree polynomial
The *linear* in linear model refers to the $\theta$ parameters, not the data.
Now, we just need to map the single $s$ parameter into a vector with different polynomial degrees, so 1 for the intercept, $s$ for the linear component, and $s^2$ for the
squared component. So $\phi(s) = (1,s,s^2)$.
We can write that as a formula:

```{r}
cars %>%
    model.matrix(dist ~ speed + speed^2, data =.) %>%
    head
```

To avoid R interpreting that multiplication as interaction terms we need to specify that `speed^2` term should be interpreted just the way it is.
We do that using the identity function, I():

```{r}
cars %>%
    model.matrix(dist ~ speed + I(speed^2), data = .) %>%
    head
```

Now the model has three columns, which is precisely what we want.

```{r}
cars %>% lm(dist ~ speed + I(speed ^ 2), data = .) %>%
    summary
```

Plotting the model:

```{r}
cars %>% ggplot(aes(x = speed, y = dist)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x + I(x^2))
```

#### Validating models

Most complext models will always fit the training data better, but they will not necessarily generalize better.
If i gave a defree that is the same as the number of data points, i can fit the data perfectly. But it will be
fitting both the systematic relationship between $x$ and $y$ and *also* the statistical errors on our targets $t$.
It might be utterly useless for predicting point number $n + 1$.

*What i really need to know is whether one or the other model is better at predicting the distance from the speed.*

**We can fit two models and get ther predictions using the `predict()` function. It takes the fitted model as the first 
argument and data to predict on as the secon:**

```{r}
line <- cars %>% lm(dist ~ speed, data = .)
poly <- cars %>% lm(dist ~ speed + I(speed^2), data = .)
predict(line, cars) %>% head
predict(poly, cars) %>% head
```

#### Evaluating Regression Models

To compare two models, we need a measure of how well they fit. Since both models are fitting the squared distances
from prediction to targets, a fair measure would be looking at the mean squared error.

```{r}
rmse <- function(x,t) sqrt(mean(sum((t - x)^2)))
rmse(predict(line, cars), cars$dist)
rmse(predict(poly, cars), cars$dist)
```

The polynomial fits slightly better, which it should based on theory, but there is a bit of a cheat here.
The more complex model might be overfitting the data and capturing statistical noise we don't want it to capture.
**What we really want to know is how well the model generalizes; how well do the work on data thet haven't already seen 
and used to fit their parameters?.

To compare models we need to have data that isn't used in the fitting.

We can split the data into two sets, one we use for training and the other we use to test the models.
There are 50 data points so I can take the first 25 to train my models on and the next 25 to test them on.

```{r}
training_data <- cars[1:25,]
test_data <- cars[26:50,]

line <- training_data %>% lm(dist ~ speed, data = .)
poly <- training_data %>% lm(dist ~ speed + I(speed^2), data = .)

rmse(predict(line, test_data), test_data$dist)
rmse(predict(poly, test_data), test_data$dist)
```

The second-degree polynomial is still better, but I am also still cheating. There is more structure in my dataset than just
the speed and distances. The data frame is sorted according to the distance so the training set has all the short distances and 
the test data all the long distances. They are not similar. That is not good.

**When you split your data into training and test data, you will want to sample data points randomly**.

We can use the `sample()` function to sample randomly zeros and ones:

```{r}
sampled_cars <- cars %>%
    mutate(training = sample(0:1, nrow(cars),replace = TRUE))

sampled_cars %>% head
```

This will be roughly half the data we get for training, not exatly 50/50 for training and test data, but it is now random:

```{r}
training_data <- sampled_cars %>% filter(training == 1)
test_data <- sampled_cars %>% filter(training == 0)

training_data %>% head
test_data %>% head
```

Now we get a better estimate of how functions are working:

```{r}
line <- training_data %>% lm(dist ~ speed, data = .)
poly <- training_data %>% lm(dist ~ speed + I(speed^2), data = .)

rmse(predict(line,test_data), test_data$dist)
rmse(predict(poly, test_data), test_data$dist)
```

#### Evaluating Classification Models

With classification you want to know how many data points are classified correctly and how many are not.

As an example, we can take the breat cancer data and fit the model:

```{r}
formatted_data <- BreastCancer %>%
    mutate(Cl.thickness.numeric = 
            as.numeric(as.character(Cl.thickness)),
           Cell.size.numeric = 
            as.numeric(as.character(Cell.size))) %>%
    mutate(IsMalignant = ifelse(Class == "benign",0,1))

fitted_model <- formatted_data %>%
    glm(IsMalignant ~ Cl.thickness.numeric +  Cell.size.numeric, data = ., family = "binomial")
```

We can again use `predict()` to get its prediction.
By default the model created with `glm()` will be "logit" units, but we can use the `type` parameter to get it in 
the input unit used with probabilities.

```{r}
predict(fitted_model, formatted_data, type = "response") %>% head
```

The natural choice here is to split the probabilities at 50%.

```{r}
classify <- function(probability) ifelse(probability < 0.5, 0, 1)
classified_malignant <- classify(predict(fitted_model, formatted_data))
classified_malignant %>% head
```

#### Confusion Matrix

If we just put the classification using the `table()` function, as follows:

```{r}
table(formatted_data$IsMalignant, classified_malignant)
```

This table, contrasting predictions against true classes, is known as the *confusion matrix*.
The rows count how many zeros and ones we see in the `formatted_data$IsMalignant` argument and 
the columns how many zeros and ones we see in the `classified_malignant`argument. So the first
row is where the data says the tumors *are not* malignant and the second row is where the data says 
that the tumors *are* malignant.

The first column is where the predictions say the tumors are not malignant while the second column
is where the predictions say that they are.

This depends on the order of the argument to `table()`, you can provide a parameter, `dnn(dimnames)` 
to make the table remember it for you.

```{r}
table(formatted_data$IsMalignant, classified_malignant,
      dnn=c("Data", "Predictions"))
```

**The correct predictions are on the diagonal, and the off-diagonal are where our model predicts incorrectly.
The first element, where the model predict that the tumor is benign and the data agrees, is called *true negatives*. The element to the right of it
where the model says a tumor is malignant but the data says it is not, is called the *false positives*.

The second row is where the data sayss the tumors are malignant. The first column is where the prediction says
that it isn't a malignant tumor, and these are called the *false negatives* 

The second column is the cases where both the model and the data says that the tumor is malignant. That is the true positives**.

The classes do not have to be zeros and ones. That was just easier in this particular model where i had to translate the classes into zeros
and ones for the logistic classification anyway. But really, the classe are "benign" and "malignant".

```{r}
classify <- function(probability)
    ifelse(probability < 0.5, "benign", "malignant")
classified <- classify(predict(fitted_model, formatted_data))

table(formatted_data$Class, classified,
      dnn=c("Data","Predictions"))
```

Summaries still will often depend on which clas we consider "positive" and which we consider "negative". so it is always worth a
thought deciding which you want to call which and definitely something you want to make explicit in any documentation of your anlaysis.


#### Accuracy

The simples measure of how well a classification is doing is the *accuracy*. It measures how many classes it gets right out the total, so it is the
diagonal values of the confusion matrix divided by the toal.

```{r}
confusion_matrix <- table(formatted_data$Class, classified,
                          dnn=c("Data","Predictions"))

(accuracy <- sum(diag(confusion_matrix)/ sum(confusion_matrix)))
```






