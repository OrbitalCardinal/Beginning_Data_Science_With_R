---
title: "Chapter 6 - Supervised Learning"
output: html_document
author: "Edson Raul Cepeda Marquez"
---

### Machine Learning

Machine Learning is the discipline of developing and applying models and algorithms for learning from data.
Machine Learning concerns learning from data, you use a generic algorithm that you feed examples of solutions
to, and let it learn how to solve the problem from those examples.

### Supervised Learning

Supervised Learning is used when you have variables you want to predict using other variables.
For the simplest case of supervised learning, we have one response variable, $y$, and one input variable, $x$, and we want to figure out 
a function, $f$, mapping the input to output, i.e., so that $y=f(x)$.
What we have to work with is example data of matching $x$ and $y$.

### Regression versus Classification

There are two types of supervised learning: regression and classification. Regression is used when the output variable we try to target is a number.
Classification is used when we try to target categorical variables.

Take linear regression, $y = \alpha x + \beta$. It is a regression because the variable we are trying to target is a number.
The parameterized class of functions $f_\theta$, are all lines. If we let $\theta = \theta_1, \theta_0$ and $\alpha = \theta_1, \beta = \theta_0$
then $y(\theta) = f(x;\theta) = \theta_1 x + \theta_0$. Fitting a linear model consist of finding the best $\theta$, where *best* is defined as the 
$\theta$ that gets **$y$**$(\theta)$ closes to **$t$**. The distance used in linear regression is the squared Euclidean distance

$$
|y^{(\theta)} - t|^2 = \sum_{i=1}^n (\theta_1 x_i + \theta_0 - t_i)^2
$$ .

For an example of classfication, assume that the targets $t_i$ are binary, encoded as 0 and 1, but that the input variables $x_i$ are still real numbers.
A common way of defininf tha mapping function $f(-;\theta)$ is to let it map $x$ to the unit interval $[0,1]$ and interpret the resulting $y(\theta)$ as 
the probability that $t$ is 1.
In a classification setting, you would then predict 0 if $f(x;\theta) < 0.5$ and predict 1 if $f(x;\theta) > 0.5$.
In linear classification , the function $f_\theta$ could look like this:

$$
f(x;\theta) = \sigma (\theta_1 x + \theta_0)
$$

Where $\sigma$ is a sigmoid function (a function mapping $R \rightarrow [0,1]$ that is "S-shaped").
A common choice of $\sigma$ is the logistic function $\sigma : z \longmapsto \frac{1}{1 + e^{-z'}}$ in which case we call the fitting $f(-;\theta)$ *logistic regression*. 

#### Linear Regression
If we take a simple linear regression, $f_\theta = \theta_1 x + \theta_0$, we need the function lm().

For example, considering the speed as the $x$ value and the distance as the $y$ for the *cars* dataset.

```{r}
library(magrittr)
library(ggplot2)
```
```{r}
cars %>% head
```

We can see that there is a very clear linear relation between speed and distance:

```{r}
cars %>% ggplot(aes(x = speed, y = dist)) +
    geom_point() +
    geom_smooth(method = "lm")
```

`geom_smooth()` will also plot the uncertainty of the fit. This is the gray area in the plot, is the area were the line is likely to be.

To actually fit the data and get information about the fit, we use the `lm()` function with the model specification, `dist ~ speed`, and we can use the `summary()` function to see information about the fit.

```{r}
cars %>% lm(dist ~ speed, data = .) %>% summary
```

Or we can use the `coefficients()` function to get the point estimates and the `confint()` function to confidence interval for the parameters.

```{r}
cars %>% lm(dist ~ speed, data = .) %>% coefficients
cars %>% lm(dist ~ speed, data = .) %>% confint
```

Here, (`Intercept`) is $\theta_0$ and `speed` is $\theta_1$.

##### Manually getting the linear regression

Plotting the lines $y = \theta_1 x$ for different choices of $\theta$
```{r}
predict_dist <- function(speed, theta_1)
    data.frame(speed = speed,
               dist = theta_1 * speed,
               theta = as.factor(theta_1))

cars %>% ggplot(aes(x = speed, y = dist, colour = theta)) +
    geom_point(colour = "black") +
    geom_line(data = predict_dist(cars$speed,2)) +
    geom_line(data = predict_dist(cars$speed,3)) +
    geom_line(data = predict_dist(cars$speed,4)) +
    scale_color_discrete(name=expression(theta[1]))

```

Plotting the error against different choices of $\theta_1$.

```{r}
thetas <- seq(0,5, length.out = 50)
fitting_error <- Vectorize(
    function(theta)
        sum((theta * cars$speed - cars$dist)**2)
)

data.frame(thetas = thetas, errors =  fitting_error(thetas)) %>%
    ggplot(aes(x = thetas, y = errors)) +
    geom_line() +
    xlab(expression(theta[1])) + ylab("")
```

#### Logistic Regression (Classification)

Consider binary clasiffication and logistic regression.
We can use the breat cancer data from the `mlbench` library and as if the clump thickness has an effect on the risk
of a tumor being malignant.
We want to see if we can predict the `Class` variable from the `Cl.thickness` variable.

```{r}
library(mlbench)
library(knitr)
data("BreastCancer")
BreastCancer %>% head
```

We can plot the data against the fit. 
Since the malignant status is either 0 or 1, the points would overlap but if we add a little jitter to the plot we can still se them,
and if we make them slightly transparent, we can see the density of the points.

```{r}
BreastCancer %>%
    ggplot(aes(x = Cl.thickness, y = Class)) +
    geom_jitter(height = 0.05, width = 0.3, alpha = 0.4)
```

For classification we still specify the prediction function $y=f(x)$ using the formula `y ~ x`.
The outcome parameter for `y ~ x` is just binary now.

The breast cancer dataset consider the clump thickness as ordered factors.
Generally, it is no advisable to directly translate categorical data into numeric data, it is okay in this case.
Using the function `as.numeric()` will do this.
Another safer aproach is to first translate the factor into strings and then into numbers.

The second problem is that the `glm()` function expects the response variable to be numerical, coding classes like 0 or 1, 
while `BreastCancer` data encodes the classes as a factor.

```{r}
library(dplyr)
BreastCancer %>%
    mutate(Cl.thickness.numeric = as.numeric(as.character(Cl.thickness))) %>% 
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    ggplot(aes(x = Cl.thickness.numeric, y = IsMalignant)) +
    geom_jitter(height = 0.05, width = 0.3, alpha = 0.4) +
    geom_smooth(method = "glm",
                method.args = list(family = "binomial"))

```

To actually get the fitted object, we use `glm()` like we used `lm()` for the linear regression.

```{r}
BreastCancer %>% 
    mutate(Cl.thickness.numeric = 
                as.numeric(as.character(Cl.thickness))) %>%
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    glm(IsMalignant ~ Cl.thickness.numeric,
        family = "binomial",
        data = .)
```

#### Model Matrices and Formula

Most statistical models and machine learning algorithm creates maps not from single value, but form a vector.
What th elinear model actually sees is a matrix $\mathbf{x}$ considering that the data to fit is  $(\mathbf{x}. \mathbf{t})$,
so we'll call that $X$.
This matrix is known as the *model matrix*. 

It has a row per value in $\mathbf{x}$ and it has two columns, on for the intercept and one for the $x$ values.

$$ 
X =
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
1 & x_3 \\
\vdots & \vdots \\
1 & x_n
\end{bmatrix}
$$

We can see what mode matrix R generates for a given dataset and formula using
`model.matrix()` function. For the `cars` data, if we want to fir `dist` versus `speed` we get this:

```{r}
cars %>%
    model.matrix(dist ~ speed, data = .) %>%
    head(5)
```

For linear regression, the map is a pretty simple one. If we let $\theta = (\theta_0, \theta_1)$ then it is 
just multiplying that with the model matrix $X$.

$$
\theta^T X = (\theta_0,\theta_1)
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
1 & x_3 \\
\vdots & \vdots \\
1 & x_n
\end{bmatrix}
= 
\begin{bmatrix}
\theta_0 + \theta_1 x_1 \\
\theta_0 + \theta_1 x_2 \\
\theta_0 + \theta_1 x_3 \\
\vdots \\
\theta_0 + \theta_1 x_n \\
\end{bmatrix}
$$

The combination of formulas and model matrices is a powerful tool for specifying model.
Since all the algorithms we use for fitting data works on model matrices anyway, there is no reason to hold back on how complex formulas to give them.
If you want to fit more than one parameter, no problem. You just give write `y ~ x + z` and the model matrix will have three columns.

$$
X =
\begin{bmatrix}
1 & x_1 & z_1 \\
1 & x_2 & z_2 \\
1 & x_3 & z_3 \\
\vdots & \vdots & \vdots \\
1 & x_n & z_n \\
\end{bmatrix}
$$

If we wanted to fit the breast cancer data to both cell thickness and cell size, we can do that just by adding both explanatory variables in the formula.

```{r}
BreastCancer %>%
    mutate(Cl.thickness.numeric =
            as.numeric(as.character(Cl.thickness)),
           Cell.size.numeric = 
            as.numeric(as.character(Cell.size))) %>%
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    model.matrix(IsMalignant ~ Cl.thickness.numeric + Cell.size.numeric,
                 data = .) %>%
    head(5)
```

Then the generalized linear model fitting function will happily work with that:

```{r}
BreastCancer %>%
    mutate(Cl.thickness.numeric =
            as.numeric(as.character(Cl.thickness)),
           Cell.size.numeric = 
            as.numeric(as.character(Cell.size))) %>%
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    glm(IsMalignant ~ Cl.thickness.numeric + Cell.size.numeric,
        family = "binomial",
        data = .)
```

If you want to include interactions between your parameters, you specify that using * instead of +:

```{r}
BreastCancer %>%
    mutate(Cl.thickness.numeric =
            as.numeric(as.character(Cl.thickness)),
           Cell.size.numeric = 
            as.numeric(as.character(Cell.size))) %>%
    mutate(IsMalignant = ifelse(Class == "benign",0,1)) %>%
    model.matrix(IsMalignant ~ Cl.thickness.numeric * Cell.size.numeric,
                 data = .) %>%
    head(5)
```

If you want to use all the variables in your data excpet the response variable, you can even use the formula `y ~ .` where the `.` wiil give tou all parameters in your data except `y`.

We can transform the data before give it to our learning algorithms. 
We use a function $\phi$. It is called phi because we call what it produces *features* of our data and the point of it is to pull
out relevant features of the data to give to the learning algorithm.
It maps from vectors to vectors, so we can use it to transform each row in your raw data into the rows of the model matrix, whigh we will then call $\phi$ instead of $X$.

$$
\phi = 
\begin{bmatrix}
- \phi (\mathbf{x}_1) - \\
- \phi (\mathbf{x}_2) - \\
- \phi (\mathbf{x}_3) - \\
\cdots \\
- \phi (\mathbf{x}_n) - \\
\end{bmatrix}
$$

To train the `cars`data but fitting a polynomial instead of a line.
Considering $d$ denotes breaking distance and $s$ the speed, then we want to fit $d = \theta_0 + \theta_1 s + \theta_2 s^2 + \cdots + \theta_n s^n$.
We'll just do $n=2$ so we want to fit a second-degree polynomial
The *linear* in linear model refers to the $\theta$ parameters, not the data.
Now, we just need to map the single $s$ parameter into a vector with different polynomial degrees, so 1 for the intercept, $s$ for the linear component, and $s^2$ for the
squared component. So $\phi(s) = (1,s,s^2)$.
We can write that as a formula:

```{r}
cars %>%
    model.matrix(dist ~ speed + speed^2, data =.) %>%
    head
```

To avoid R interpreting that multiplication as interaction terms we need to specify that `speed^2` term should be interpreted just the way it is.
We do that using the identity function, I():

```{r}
cars %>%
    model.matrix(dist ~ speed + I(speed^2), data = .) %>%
    head
```

Now the model has three columns, which is precisely what we want.

```{r}
cars %>% lm(dist ~ speed + I(speed ^ 2), data = .) %>%
    summary %>% 
```

Plotting the model:

```{r}
cars %>% ggplot(aes(x = speed, y = dist)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x + I(x^2))
```
